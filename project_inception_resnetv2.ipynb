{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTAzLsG2MreQ"
      },
      "source": [
        "# importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from random import randint\n",
        "from IPython.display import SVG\n",
        "import matplotlib.gridspec as gridspec\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.image as mpimg\n",
        "import keras\n",
        "import glob\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWQzjyqYNDzd"
      },
      "source": [
        "# importing the required libraries\n",
        "\n",
        "from os import listdir\n",
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fkep0CHND17"
      },
      "source": [
        "# importing the required libraries\n",
        "\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense, Flatten, Bidirectional, Dropout\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXKalsx6ND4h"
      },
      "source": [
        "# Loading and preprocessing the image with InceptionResNetV2\n",
        "images = []\n",
        "files = listdir('/content/drive/MyDrive/HTML/images/')\n",
        "files.sort()\n",
        "for filename in files:\n",
        "    images.append(img_to_array(load_img('/content/drive/MyDrive/HTML/images/'+filename, target_size=(299, 299))))\n",
        "images = np.array(images, dtype=float)\n",
        "images = preprocess_input(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWYB9MTLND7O"
      },
      "source": [
        "# Run the images through inception-resnet \n",
        "IR2 = InceptionResNetV2(weights='imagenet', include_top=False)\n",
        "features = IR2.predict(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpYAxkgrEyk",
        "outputId": "3e24fc7f-8e5e-4b74-989d-8d76e61a66c0"
      },
      "source": [
        "# visualizing the extracted feature shape\n",
        "\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 8, 8, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY9Z5dunND9h"
      },
      "source": [
        "# This piece of code is referred from the website \"https://www.programmersought.com/article/3918650197/\"\n",
        "\n",
        "\n",
        "maximum_caption_length = 100\n",
        "token = Tokenizer(filters='', split=\" \", lower=False)\n",
        "\n",
        "# Function to read the document\n",
        "def read_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# Loading the HTML files\n",
        "X = []\n",
        "all_files = listdir('/content/drive/MyDrive/HTML/html/')\n",
        "all_files.sort()\n",
        "for filename in all_files:\n",
        "    X.append(read_file('/content/drive/MyDrive/HTML/html/'+filename))\n",
        "\n",
        "# Creating the vocabulary from html files\n",
        "token.fit_on_texts(X)\n",
        "\n",
        "# Add +1 to leave space for empty words\n",
        "vocab_size = len(token.word_index) + 1\n",
        "\n",
        "sequences = token.texts_to_sequences(X)\n",
        "max_length = max(len(s) for s in sequences)\n",
        "\n",
        "X, y, image_data = list(), list(), list()\n",
        "for img_no, seq in enumerate(sequences):\n",
        "    for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        # Padding the sentences, if the sentence is short\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        image_data.append(ext_features[img_no])\n",
        "        X.append(in_seq[-100:])\n",
        "        y.append(out_seq)\n",
        "\n",
        "X, y, image_data = np.array(X), np.array(y), np.array(image_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86FtMEGNxtC"
      },
      "source": [
        "# Building the model\n",
        "# Importing the regularizers\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "# Create the encoder\n",
        "image = Input(shape=(9, 9, 1920,))\n",
        "image_flatten = Flatten()(image)\n",
        "image_flatten = Dense(128, activation='relu')(image_flatten)\n",
        "rep_vec = RepeatVector(maximum_caption_length)(image_flatten)\n",
        "\n",
        "language_input = Input(shape=(maximum_caption_length,))\n",
        "language = Embedding(vocab_size, 200, input_length = maximum_caption_length)(language_input)\n",
        "language = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences=True))(language)\n",
        "language = Dropout(0.3)(language)\n",
        "language = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences = True))(language)\n",
        "language = Dropout(0.3)(language)\n",
        "language = Bidirectional(LSTM(64, return_sequences=True))(language)\n",
        "language = TimeDistributed(Dense(32, activation='relu'))(language)\n",
        "\n",
        "# Create the decoder\n",
        "decoder = concatenate([rep_vec, language])\n",
        "decoder = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences=False))(decoder)\n",
        "decoder_output = Dense(vocab_size, activation='softmax')(decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyWChlouNxyN"
      },
      "source": [
        "# Compile the model\n",
        "model = Model(inputs=[image, language_input], outputs=decoder_output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_SNAE_1Nx0l",
        "outputId": "3404f515-d790-4576-f988-9f0d07376495"
      },
      "source": [
        "# Train the model\n",
        "model.fit([image_data, X], y, batch_size=64, shuffle=False, epochs=150, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "29/29 [==============================] - 103s 3s/step - loss: 10.6074 - val_loss: 8.8711\n",
            "Epoch 2/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 7.4024 - val_loss: 7.3606\n",
            "Epoch 3/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 6.0744 - val_loss: 6.8145\n",
            "Epoch 4/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.6200 - val_loss: 6.7928\n",
            "Epoch 5/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.5156 - val_loss: 6.9891\n",
            "Epoch 6/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.4651 - val_loss: 7.1308\n",
            "Epoch 7/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.4676 - val_loss: 7.2620\n",
            "Epoch 8/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.4266 - val_loss: 7.6094\n",
            "Epoch 9/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.4319 - val_loss: 7.3970\n",
            "Epoch 10/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.4370 - val_loss: 7.4749\n",
            "Epoch 11/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.3918 - val_loss: 8.0591\n",
            "Epoch 12/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.4124 - val_loss: 7.9505\n",
            "Epoch 13/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.3364 - val_loss: 8.1448\n",
            "Epoch 14/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2979 - val_loss: 8.1547\n",
            "Epoch 15/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2816 - val_loss: 8.1469\n",
            "Epoch 16/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.2582 - val_loss: 8.1082\n",
            "Epoch 17/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2528 - val_loss: 8.3353\n",
            "Epoch 18/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2721 - val_loss: 8.2867\n",
            "Epoch 19/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2443 - val_loss: 8.2181\n",
            "Epoch 20/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2290 - val_loss: 8.2003\n",
            "Epoch 21/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.2202 - val_loss: 8.0630\n",
            "Epoch 22/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2075 - val_loss: 8.2477\n",
            "Epoch 23/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1922 - val_loss: 8.4657\n",
            "Epoch 24/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2423 - val_loss: 8.5765\n",
            "Epoch 25/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2223 - val_loss: 8.4211\n",
            "Epoch 26/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2048 - val_loss: 8.3874\n",
            "Epoch 27/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1900 - val_loss: 8.3044\n",
            "Epoch 28/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1781 - val_loss: 8.5947\n",
            "Epoch 29/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2500 - val_loss: 8.4743\n",
            "Epoch 30/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1789 - val_loss: 8.2751\n",
            "Epoch 31/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2005 - val_loss: 8.3146\n",
            "Epoch 32/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.1623 - val_loss: 8.4508\n",
            "Epoch 33/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2193 - val_loss: 8.5519\n",
            "Epoch 34/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2536 - val_loss: 8.1204\n",
            "Epoch 35/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2246 - val_loss: 8.2145\n",
            "Epoch 36/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1955 - val_loss: 8.4600\n",
            "Epoch 37/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1855 - val_loss: 8.3041\n",
            "Epoch 38/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1872 - val_loss: 8.9116\n",
            "Epoch 39/150\n",
            "29/29 [==============================] - 97s 3s/step - loss: 5.1613 - val_loss: 8.8323\n",
            "Epoch 40/150\n",
            "29/29 [==============================] - 95s 3s/step - loss: 5.2511 - val_loss: 8.5837\n",
            "Epoch 41/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1864 - val_loss: 8.4109\n",
            "Epoch 42/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1991 - val_loss: 8.7095\n",
            "Epoch 43/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.1564 - val_loss: 8.7844\n",
            "Epoch 44/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1520 - val_loss: 8.5001\n",
            "Epoch 45/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1699 - val_loss: 8.5259\n",
            "Epoch 46/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2524 - val_loss: 8.8216\n",
            "Epoch 47/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1821 - val_loss: 8.5913\n",
            "Epoch 48/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1678 - val_loss: 8.4887\n",
            "Epoch 49/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1730 - val_loss: 8.6362\n",
            "Epoch 50/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2001 - val_loss: 8.5122\n",
            "Epoch 51/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.2191 - val_loss: 8.3026\n",
            "Epoch 52/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1893 - val_loss: 8.5852\n",
            "Epoch 53/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1662 - val_loss: 8.8003\n",
            "Epoch 54/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1499 - val_loss: 8.6198\n",
            "Epoch 55/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1458 - val_loss: 8.7524\n",
            "Epoch 56/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1609 - val_loss: 9.0309\n",
            "Epoch 57/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.1520 - val_loss: 8.7929\n",
            "Epoch 58/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1750 - val_loss: 8.8182\n",
            "Epoch 59/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1678 - val_loss: 8.7553\n",
            "Epoch 60/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1568 - val_loss: 8.5486\n",
            "Epoch 61/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.2817 - val_loss: 8.8039\n",
            "Epoch 62/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2029 - val_loss: 8.5252\n",
            "Epoch 63/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1599 - val_loss: 8.4495\n",
            "Epoch 64/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1732 - val_loss: 8.3928\n",
            "Epoch 65/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.2001 - val_loss: 8.4274\n",
            "Epoch 66/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.2393 - val_loss: 8.1785\n",
            "Epoch 67/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1916 - val_loss: 8.8864\n",
            "Epoch 68/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2098 - val_loss: 8.4178\n",
            "Epoch 69/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.1751 - val_loss: 8.4094\n",
            "Epoch 70/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1702 - val_loss: 8.8428\n",
            "Epoch 71/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.2319 - val_loss: 8.6404\n",
            "Epoch 72/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1692 - val_loss: 8.7176\n",
            "Epoch 73/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1623 - val_loss: 8.4778\n",
            "Epoch 74/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.1718 - val_loss: 8.6205\n",
            "Epoch 75/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1934 - val_loss: 8.6405\n",
            "Epoch 76/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1713 - val_loss: 8.6032\n",
            "Epoch 77/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1937 - val_loss: 8.3610\n",
            "Epoch 78/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1894 - val_loss: 8.2786\n",
            "Epoch 79/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1597 - val_loss: 8.5002\n",
            "Epoch 80/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1951 - val_loss: 8.1964\n",
            "Epoch 81/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1775 - val_loss: 8.4266\n",
            "Epoch 82/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1791 - val_loss: 8.7911\n",
            "Epoch 83/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1722 - val_loss: 8.4026\n",
            "Epoch 84/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1816 - val_loss: 8.4875\n",
            "Epoch 85/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.1763 - val_loss: 8.4730\n",
            "Epoch 86/150\n",
            "29/29 [==============================] - 90s 3s/step - loss: 5.1436 - val_loss: 8.4764\n",
            "Epoch 87/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1774 - val_loss: 8.8565\n",
            "Epoch 88/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1608 - val_loss: 8.4495\n",
            "Epoch 89/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1747 - val_loss: 8.4747\n",
            "Epoch 90/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1579 - val_loss: 8.6239\n",
            "Epoch 91/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1459 - val_loss: 8.6502\n",
            "Epoch 92/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1455 - val_loss: 8.2172\n",
            "Epoch 93/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1480 - val_loss: 8.5698\n",
            "Epoch 94/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1354 - val_loss: 8.6202\n",
            "Epoch 95/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1831 - val_loss: 8.5828\n",
            "Epoch 96/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1470 - val_loss: 8.3944\n",
            "Epoch 97/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1502 - val_loss: 8.3711\n",
            "Epoch 98/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1296 - val_loss: 8.3033\n",
            "Epoch 99/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1913 - val_loss: 8.8521\n",
            "Epoch 100/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1530 - val_loss: 8.4634\n",
            "Epoch 101/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1326 - val_loss: 8.5686\n",
            "Epoch 102/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1763 - val_loss: 8.4755\n",
            "Epoch 103/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1401 - val_loss: 8.7222\n",
            "Epoch 104/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1496 - val_loss: 8.8263\n",
            "Epoch 105/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1527 - val_loss: 8.6643\n",
            "Epoch 106/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1335 - val_loss: 8.5597\n",
            "Epoch 107/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1388 - val_loss: 8.7997\n",
            "Epoch 108/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1744 - val_loss: 8.3066\n",
            "Epoch 109/150\n",
            "29/29 [==============================] - 94s 3s/step - loss: 5.1563 - val_loss: 8.7793\n",
            "Epoch 110/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1312 - val_loss: 8.7360\n",
            "Epoch 111/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1917 - val_loss: 8.8260\n",
            "Epoch 112/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1973 - val_loss: 8.8477\n",
            "Epoch 113/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2038 - val_loss: 8.8816\n",
            "Epoch 114/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1528 - val_loss: 8.8381\n",
            "Epoch 115/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1573 - val_loss: 8.7850\n",
            "Epoch 116/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1955 - val_loss: 8.8165\n",
            "Epoch 117/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.2340 - val_loss: 8.6208\n",
            "Epoch 118/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1663 - val_loss: 8.7473\n",
            "Epoch 119/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1510 - val_loss: 8.5625\n",
            "Epoch 120/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1413 - val_loss: 8.9274\n",
            "Epoch 121/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1417 - val_loss: 8.9183\n",
            "Epoch 122/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1575 - val_loss: 8.7566\n",
            "Epoch 123/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1738 - val_loss: 9.1470\n",
            "Epoch 124/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1518 - val_loss: 8.9948\n",
            "Epoch 125/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.1604 - val_loss: 9.2065\n",
            "Epoch 126/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1478 - val_loss: 9.0543\n",
            "Epoch 127/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1561 - val_loss: 8.9225\n",
            "Epoch 128/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1809 - val_loss: 8.8837\n",
            "Epoch 129/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1543 - val_loss: 9.1443\n",
            "Epoch 130/150\n",
            "29/29 [==============================] - 93s 3s/step - loss: 5.2270 - val_loss: 9.0447\n",
            "Epoch 131/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1297 - val_loss: 9.0969\n",
            "Epoch 132/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1686 - val_loss: 9.0404\n",
            "Epoch 133/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1977 - val_loss: 9.0665\n",
            "Epoch 134/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1948 - val_loss: 9.0669\n",
            "Epoch 135/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1459 - val_loss: 9.0910\n",
            "Epoch 136/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1381 - val_loss: 9.2291\n",
            "Epoch 137/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1655 - val_loss: 9.2333\n",
            "Epoch 138/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1395 - val_loss: 9.0915\n",
            "Epoch 139/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1433 - val_loss: 9.3222\n",
            "Epoch 140/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1175 - val_loss: 9.1220\n",
            "Epoch 141/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.1173 - val_loss: 9.2135\n",
            "Epoch 142/150\n",
            "29/29 [==============================] - 92s 3s/step - loss: 5.2385 - val_loss: 9.2388\n",
            "Epoch 143/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1613 - val_loss: 9.1382\n",
            "Epoch 144/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1575 - val_loss: 9.1706\n",
            "Epoch 145/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1850 - val_loss: 9.2208\n",
            "Epoch 146/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1249 - val_loss: 9.3556\n",
            "Epoch 147/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.2111 - val_loss: 9.0237\n",
            "Epoch 148/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1598 - val_loss: 9.0317\n",
            "Epoch 149/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1337 - val_loss: 9.1987\n",
            "Epoch 150/150\n",
            "29/29 [==============================] - 91s 3s/step - loss: 5.1645 - val_loss: 9.2231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7d9115ad90>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXahEToVNx3n"
      },
      "source": [
        "# This piece of code is referred from the website \"https://www.programmersought.com/article/3918650197/\"\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh-qqMbVNx6b"
      },
      "source": [
        "# This piece of code is referred from the website \"https://www.programmersought.com/article/3918650197/\"\n",
        "\n",
        "# generate a description for an image\n",
        "def description(model, tokenizer, photo, maximum_length):\n",
        "    in_text = 'START'\n",
        "    for i in range(900):\n",
        "        seq = tokenizer.texts_to_sequences([in_text])[0][-100:]\n",
        "        seq = pad_sequences([seq], maxlen=maximum_length)\n",
        "        yhat = model.predict([photo,seq], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        \n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        print(' ' + word, end='')\n",
        "        if word == 'END':\n",
        "            break\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I03HcNGWNEF3",
        "outputId": "b261b4db-3e38-472c-81f4-44ca78031f4a"
      },
      "source": [
        "# Testing \n",
        "test = img_to_array(load_img('/content/drive/MyDrive/HTML/images/88.jpg', target_size=(299, 299)))\n",
        "test = np.array(test, dtype=float)\n",
        "test = preprocess_input(test)\n",
        "test_features = densenet.predict(np.array([test]))\n",
        "description(model, token, np.array(test_features), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}