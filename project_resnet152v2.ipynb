{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGMA8RJEyRzT"
      },
      "source": [
        "# importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from random import randint\n",
        "from IPython.display import SVG\n",
        "import matplotlib.gridspec as gridspec\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.image as mpimg\n",
        "import keras\n",
        "import glob\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoGN3e9Eyl9f"
      },
      "source": [
        "# importing the libraries\n",
        "\n",
        "from os import listdir\n",
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1cBIOdByl_7"
      },
      "source": [
        "# importing the libraries\n",
        "\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense, Flatten, GRU, Bidirectional, Dropout\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0YTvBVDymCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9cf6a4-9c44-429c-8f21-30cda6a07b88"
      },
      "source": [
        "# Loading and preprocessing the images using ResNet152V2 architecture\n",
        "\n",
        "images = []\n",
        "files = listdir('/content/drive/MyDrive/HTML/images/')\n",
        "files.sort()\n",
        "for filename in files:\n",
        "    images.append(img_to_array(load_img('/content/drive/MyDrive/HTML/images/'+filename, target_size=(299, 299))))\n",
        "images = np.array(images, dtype=float)\n",
        "images = preprocess_input(images)\n",
        "\n",
        "# Run the images through resnet152V2 \n",
        "resnet = ResNet152V2(weights='imagenet', include_top=False)\n",
        "features = resnet.predict(images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 2s 0us/step\n",
            "234561536/234545216 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ryvqg8gFEi1",
        "outputId": "f6aaa16f-95f5-458e-c292-4926347e82a0"
      },
      "source": [
        "# printing the shape of the extracted features\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 10, 10, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fdK7z4rymEz"
      },
      "source": [
        "# This piece of code is referred from the website \"https://www.programmersought.com/article/3918650197/\"\n",
        "\n",
        "maximum_caption_length = 100\n",
        "token = Tokenizer(filters='', split=\" \", lower=False)\n",
        "\n",
        "# Function to read the document\n",
        "def read_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# Loading the HTML files\n",
        "X = []\n",
        "all_files = listdir('/content/drive/MyDrive/dataset_new1.3/html/')\n",
        "all_files.sort()\n",
        "for filename in all_files:\n",
        "    X.append(read_file('/content/drive/MyDrive/dataset_new1.3/html/'+filename))\n",
        "\n",
        "# Creating the vocabulary from html files\n",
        "token.fit_on_texts(X)\n",
        "\n",
        "# Add +1 to leave space for empty words\n",
        "vocab_size = len(token.word_index) + 1\n",
        "\n",
        "sequences = token.texts_to_sequences(X)\n",
        "max_length = max(len(s) for s in sequences)\n",
        "\n",
        "X, y, image_data = list(), list(), list()\n",
        "for img_no, seq in enumerate(sequences):\n",
        "    for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        # Padding the sentences, if the sentence is short\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        image_data.append(ext_features[img_no])\n",
        "        X.append(in_seq[-100:])\n",
        "        y.append(out_seq)\n",
        "\n",
        "X, y, image_data = np.array(X), np.array(y), np.array(image_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5z9a51PymHO"
      },
      "source": [
        "# Model building\n",
        "\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "# Create the encoder\n",
        "image = Input(shape=(9, 9, 1920,))\n",
        "image_flatten = Flatten()(image)\n",
        "image_flatten = Dense(128, activation='relu')(image_flatten)\n",
        "rep_vec = RepeatVector(maximum_caption_length)(image_flatten)\n",
        "\n",
        "language_input = Input(shape=(maximum_caption_length,))\n",
        "language = Embedding(vocab_size, 200, input_length=maximum_caption_length)(language_input)\n",
        "language = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences=True))(language)\n",
        "language = Dropout(0.3)(language)\n",
        "language = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences = True))(language)\n",
        "language = Dropout(0.3)(language)\n",
        "language = Bidirectional(LSTM(64, return_sequences=True))(language)\n",
        "language = TimeDistributed(Dense(32, activation='relu'))(language)\n",
        "\n",
        "# Create the decoder\n",
        "decoder = concatenate([rep_vec, language])\n",
        "decoder = Bidirectional(LSTM(64, kernel_regularizer = l2(0.01), return_sequences=False))(decoder)\n",
        "decoder_output = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7yRsmHa9nor",
        "outputId": "45dc3b54-94dc-4378-c99b-85ce5bae2fc1"
      },
      "source": [
        "# Compile the model\n",
        "import tensorflow \n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "model = Model(inputs=[image, language_input], outputs=decoder_output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.RMSprop(lr = 0.001, decay = 0.001))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NK_HFqxymJq",
        "outputId": "eb3fbcde-95e8-4b6d-f25c-8ed159ac9145"
      },
      "source": [
        "# Train the neural network\n",
        "history = model.fit([image_data, X], y, batch_size=64, shuffle=False, epochs=200, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "29/29 [==============================] - 27s 431ms/step - loss: 16.3412 - val_loss: 13.1573\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - 8s 278ms/step - loss: 10.9045 - val_loss: 9.2650\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 7.6780 - val_loss: 7.0957\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 6.1077 - val_loss: 6.3595\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - 8s 277ms/step - loss: 5.8003 - val_loss: 6.3860\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - 8s 278ms/step - loss: 5.7308 - val_loss: 6.4046\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.7040 - val_loss: 6.5048\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.6611 - val_loss: 6.4854\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - 8s 274ms/step - loss: 5.6509 - val_loss: 6.6088\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.6454 - val_loss: 6.5525\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.6279 - val_loss: 6.5858\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - 8s 277ms/step - loss: 5.5998 - val_loss: 6.6407\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.5896 - val_loss: 6.6840\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.5794 - val_loss: 6.6331\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - 8s 280ms/step - loss: 5.5766 - val_loss: 6.7052\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - 8s 277ms/step - loss: 5.5268 - val_loss: 6.6378\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - 8s 273ms/step - loss: 5.5024 - val_loss: 6.7375\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.4800 - val_loss: 6.7068\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.4388 - val_loss: 6.7301\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.4174 - val_loss: 6.8977\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.3986 - val_loss: 6.9302\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.3885 - val_loss: 6.7342\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.4013 - val_loss: 6.7497\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - 8s 272ms/step - loss: 5.3815 - val_loss: 6.7557\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - 8s 261ms/step - loss: 5.3765 - val_loss: 6.7377\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.3682 - val_loss: 6.8031\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.3326 - val_loss: 6.8257\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.3269 - val_loss: 6.8919\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.3152 - val_loss: 6.8967\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.2886 - val_loss: 6.8622\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.2915 - val_loss: 6.9949\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.2950 - val_loss: 7.0266\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.2789 - val_loss: 6.9068\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.2694 - val_loss: 6.8471\n",
            "Epoch 35/200\n",
            "29/29 [==============================] - 7s 252ms/step - loss: 5.2552 - val_loss: 7.0438\n",
            "Epoch 36/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.2764 - val_loss: 6.9402\n",
            "Epoch 37/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.2431 - val_loss: 7.0271\n",
            "Epoch 38/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.2264 - val_loss: 6.9716\n",
            "Epoch 39/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.2338 - val_loss: 6.9267\n",
            "Epoch 40/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.2124 - val_loss: 6.8136\n",
            "Epoch 41/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.2234 - val_loss: 6.9738\n",
            "Epoch 42/200\n",
            "29/29 [==============================] - 7s 252ms/step - loss: 5.2263 - val_loss: 7.0030\n",
            "Epoch 43/200\n",
            "29/29 [==============================] - 7s 251ms/step - loss: 5.2088 - val_loss: 7.0263\n",
            "Epoch 44/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.2129 - val_loss: 6.9281\n",
            "Epoch 45/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1976 - val_loss: 7.0583\n",
            "Epoch 46/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.1951 - val_loss: 7.1252\n",
            "Epoch 47/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.2015 - val_loss: 7.1095\n",
            "Epoch 48/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1865 - val_loss: 7.1947\n",
            "Epoch 49/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1861 - val_loss: 7.0884\n",
            "Epoch 50/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1929 - val_loss: 7.1497\n",
            "Epoch 51/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1798 - val_loss: 7.0256\n",
            "Epoch 52/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.1693 - val_loss: 6.9238\n",
            "Epoch 53/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1713 - val_loss: 7.0963\n",
            "Epoch 54/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.1706 - val_loss: 7.1012\n",
            "Epoch 55/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1717 - val_loss: 7.1278\n",
            "Epoch 56/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.1568 - val_loss: 7.1231\n",
            "Epoch 57/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1625 - val_loss: 7.2484\n",
            "Epoch 58/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1643 - val_loss: 7.0130\n",
            "Epoch 59/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1629 - val_loss: 7.1260\n",
            "Epoch 60/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1535 - val_loss: 6.9774\n",
            "Epoch 61/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1575 - val_loss: 7.1627\n",
            "Epoch 62/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1388 - val_loss: 6.8787\n",
            "Epoch 63/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1499 - val_loss: 7.0158\n",
            "Epoch 64/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1438 - val_loss: 7.1722\n",
            "Epoch 65/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1359 - val_loss: 7.0821\n",
            "Epoch 66/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.1491 - val_loss: 6.9402\n",
            "Epoch 67/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1499 - val_loss: 7.1984\n",
            "Epoch 68/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.1319 - val_loss: 7.1262\n",
            "Epoch 69/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1412 - val_loss: 7.1554\n",
            "Epoch 70/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1391 - val_loss: 7.3542\n",
            "Epoch 71/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1276 - val_loss: 7.1011\n",
            "Epoch 72/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1385 - val_loss: 7.1207\n",
            "Epoch 73/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1406 - val_loss: 7.2781\n",
            "Epoch 74/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1231 - val_loss: 7.3062\n",
            "Epoch 75/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1291 - val_loss: 7.0973\n",
            "Epoch 76/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1189 - val_loss: 7.1606\n",
            "Epoch 77/200\n",
            "29/29 [==============================] - 7s 251ms/step - loss: 5.1349 - val_loss: 7.2372\n",
            "Epoch 78/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1201 - val_loss: 7.3069\n",
            "Epoch 79/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1122 - val_loss: 7.3222\n",
            "Epoch 80/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.1095 - val_loss: 7.2989\n",
            "Epoch 81/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1088 - val_loss: 7.3616\n",
            "Epoch 82/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.1111 - val_loss: 7.2927\n",
            "Epoch 83/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.1173 - val_loss: 7.1509\n",
            "Epoch 84/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.1155 - val_loss: 7.1605\n",
            "Epoch 85/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.1199 - val_loss: 7.2531\n",
            "Epoch 86/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.1115 - val_loss: 7.2476\n",
            "Epoch 87/200\n",
            "29/29 [==============================] - 8s 274ms/step - loss: 5.1143 - val_loss: 7.3816\n",
            "Epoch 88/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.1145 - val_loss: 7.3464\n",
            "Epoch 89/200\n",
            "29/29 [==============================] - 8s 273ms/step - loss: 5.1043 - val_loss: 7.3700\n",
            "Epoch 90/200\n",
            "29/29 [==============================] - 8s 273ms/step - loss: 5.0978 - val_loss: 7.1846\n",
            "Epoch 91/200\n",
            "29/29 [==============================] - 8s 274ms/step - loss: 5.1015 - val_loss: 7.1962\n",
            "Epoch 92/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.0985 - val_loss: 7.1939\n",
            "Epoch 93/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.0979 - val_loss: 7.2922\n",
            "Epoch 94/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.0868 - val_loss: 7.3836\n",
            "Epoch 95/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.0914 - val_loss: 7.2958\n",
            "Epoch 96/200\n",
            "29/29 [==============================] - 8s 274ms/step - loss: 5.0921 - val_loss: 7.4600\n",
            "Epoch 97/200\n",
            "29/29 [==============================] - 8s 274ms/step - loss: 5.0918 - val_loss: 7.3777\n",
            "Epoch 98/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0877 - val_loss: 7.3413\n",
            "Epoch 99/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0906 - val_loss: 7.4489\n",
            "Epoch 100/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0815 - val_loss: 7.3828\n",
            "Epoch 101/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0866 - val_loss: 7.1212\n",
            "Epoch 102/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0903 - val_loss: 7.4204\n",
            "Epoch 103/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0859 - val_loss: 7.3720\n",
            "Epoch 104/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0802 - val_loss: 7.2593\n",
            "Epoch 105/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0854 - val_loss: 7.3720\n",
            "Epoch 106/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0847 - val_loss: 7.3810\n",
            "Epoch 107/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0805 - val_loss: 7.2560\n",
            "Epoch 108/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0923 - val_loss: 7.3838\n",
            "Epoch 109/200\n",
            "29/29 [==============================] - 8s 267ms/step - loss: 5.0865 - val_loss: 7.2991\n",
            "Epoch 110/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0808 - val_loss: 7.2886\n",
            "Epoch 111/200\n",
            "29/29 [==============================] - 8s 266ms/step - loss: 5.0785 - val_loss: 7.0754\n",
            "Epoch 112/200\n",
            "29/29 [==============================] - 8s 267ms/step - loss: 5.0771 - val_loss: 7.2589\n",
            "Epoch 113/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0791 - val_loss: 7.2045\n",
            "Epoch 114/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0859 - val_loss: 7.3125\n",
            "Epoch 115/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0789 - val_loss: 7.3447\n",
            "Epoch 116/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0813 - val_loss: 7.2842\n",
            "Epoch 117/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0780 - val_loss: 7.2399\n",
            "Epoch 118/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0790 - val_loss: 7.4292\n",
            "Epoch 119/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0727 - val_loss: 7.3339\n",
            "Epoch 120/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0697 - val_loss: 7.3909\n",
            "Epoch 121/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0720 - val_loss: 7.3634\n",
            "Epoch 122/200\n",
            "29/29 [==============================] - 8s 266ms/step - loss: 5.0705 - val_loss: 7.4018\n",
            "Epoch 123/200\n",
            "29/29 [==============================] - 8s 263ms/step - loss: 5.0697 - val_loss: 7.3672\n",
            "Epoch 124/200\n",
            "29/29 [==============================] - 8s 267ms/step - loss: 5.0713 - val_loss: 7.1608\n",
            "Epoch 125/200\n",
            "29/29 [==============================] - 8s 263ms/step - loss: 5.0738 - val_loss: 7.3889\n",
            "Epoch 126/200\n",
            "29/29 [==============================] - 8s 265ms/step - loss: 5.0751 - val_loss: 7.4455\n",
            "Epoch 127/200\n",
            "29/29 [==============================] - 8s 261ms/step - loss: 5.0714 - val_loss: 7.3981\n",
            "Epoch 128/200\n",
            "29/29 [==============================] - 8s 263ms/step - loss: 5.0737 - val_loss: 7.3799\n",
            "Epoch 129/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0694 - val_loss: 7.4475\n",
            "Epoch 130/200\n",
            "29/29 [==============================] - 8s 265ms/step - loss: 5.0688 - val_loss: 7.4286\n",
            "Epoch 131/200\n",
            "29/29 [==============================] - 8s 266ms/step - loss: 5.0675 - val_loss: 7.4995\n",
            "Epoch 132/200\n",
            "29/29 [==============================] - 8s 264ms/step - loss: 5.0668 - val_loss: 7.4515\n",
            "Epoch 133/200\n",
            "29/29 [==============================] - 8s 265ms/step - loss: 5.0687 - val_loss: 7.5308\n",
            "Epoch 134/200\n",
            "29/29 [==============================] - 8s 263ms/step - loss: 5.0657 - val_loss: 7.4613\n",
            "Epoch 135/200\n",
            "29/29 [==============================] - 8s 267ms/step - loss: 5.0659 - val_loss: 7.5123\n",
            "Epoch 136/200\n",
            "29/29 [==============================] - 8s 267ms/step - loss: 5.0643 - val_loss: 7.4753\n",
            "Epoch 137/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0665 - val_loss: 7.5530\n",
            "Epoch 138/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0660 - val_loss: 7.5064\n",
            "Epoch 139/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0646 - val_loss: 7.5716\n",
            "Epoch 140/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0647 - val_loss: 7.5524\n",
            "Epoch 141/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.0649 - val_loss: 7.5062\n",
            "Epoch 142/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0626 - val_loss: 7.6191\n",
            "Epoch 143/200\n",
            "29/29 [==============================] - 7s 260ms/step - loss: 5.0617 - val_loss: 7.6065\n",
            "Epoch 144/200\n",
            "29/29 [==============================] - 7s 260ms/step - loss: 5.0626 - val_loss: 7.5759\n",
            "Epoch 145/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.0670 - val_loss: 7.6401\n",
            "Epoch 146/200\n",
            "29/29 [==============================] - 8s 261ms/step - loss: 5.0616 - val_loss: 7.6012\n",
            "Epoch 147/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0609 - val_loss: 7.5686\n",
            "Epoch 148/200\n",
            "29/29 [==============================] - 8s 262ms/step - loss: 5.0638 - val_loss: 7.5295\n",
            "Epoch 149/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0613 - val_loss: 7.5780\n",
            "Epoch 150/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0618 - val_loss: 7.5924\n",
            "Epoch 151/200\n",
            "29/29 [==============================] - 8s 262ms/step - loss: 5.0636 - val_loss: 7.6156\n",
            "Epoch 152/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0622 - val_loss: 7.6096\n",
            "Epoch 153/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0603 - val_loss: 7.6072\n",
            "Epoch 154/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0605 - val_loss: 7.6427\n",
            "Epoch 155/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0593 - val_loss: 7.6436\n",
            "Epoch 156/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0596 - val_loss: 7.6553\n",
            "Epoch 157/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.0595 - val_loss: 7.6743\n",
            "Epoch 158/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0629 - val_loss: 7.6543\n",
            "Epoch 159/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0660 - val_loss: 7.6614\n",
            "Epoch 160/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0644 - val_loss: 7.6307\n",
            "Epoch 161/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0627 - val_loss: 7.6383\n",
            "Epoch 162/200\n",
            "29/29 [==============================] - 7s 260ms/step - loss: 5.0601 - val_loss: 7.6089\n",
            "Epoch 163/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0599 - val_loss: 7.6292\n",
            "Epoch 164/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0597 - val_loss: 7.6470\n",
            "Epoch 165/200\n",
            "29/29 [==============================] - 8s 261ms/step - loss: 5.0605 - val_loss: 7.6384\n",
            "Epoch 166/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0591 - val_loss: 7.6252\n",
            "Epoch 167/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0612 - val_loss: 7.6682\n",
            "Epoch 168/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0592 - val_loss: 7.6669\n",
            "Epoch 169/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0585 - val_loss: 7.7169\n",
            "Epoch 170/200\n",
            "29/29 [==============================] - 7s 259ms/step - loss: 5.0590 - val_loss: 7.6684\n",
            "Epoch 171/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0584 - val_loss: 7.6851\n",
            "Epoch 172/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.0592 - val_loss: 7.6656\n",
            "Epoch 173/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0602 - val_loss: 7.6841\n",
            "Epoch 174/200\n",
            "29/29 [==============================] - 7s 257ms/step - loss: 5.0587 - val_loss: 7.7003\n",
            "Epoch 175/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0579 - val_loss: 7.7142\n",
            "Epoch 176/200\n",
            "29/29 [==============================] - 7s 258ms/step - loss: 5.0575 - val_loss: 7.7098\n",
            "Epoch 177/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.0565 - val_loss: 7.6938\n",
            "Epoch 178/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.0559 - val_loss: 7.6875\n",
            "Epoch 179/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.0569 - val_loss: 7.6818\n",
            "Epoch 180/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.0645 - val_loss: 7.5597\n",
            "Epoch 181/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.0602 - val_loss: 7.5557\n",
            "Epoch 182/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.0582 - val_loss: 7.6214\n",
            "Epoch 183/200\n",
            "29/29 [==============================] - 7s 249ms/step - loss: 5.0575 - val_loss: 7.6414\n",
            "Epoch 184/200\n",
            "29/29 [==============================] - 7s 256ms/step - loss: 5.0555 - val_loss: 7.6089\n",
            "Epoch 185/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.0571 - val_loss: 7.6032\n",
            "Epoch 186/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.0557 - val_loss: 7.5638\n",
            "Epoch 187/200\n",
            "29/29 [==============================] - 7s 254ms/step - loss: 5.0543 - val_loss: 7.5935\n",
            "Epoch 188/200\n",
            "29/29 [==============================] - 7s 253ms/step - loss: 5.0536 - val_loss: 7.5579\n",
            "Epoch 189/200\n",
            "29/29 [==============================] - 7s 255ms/step - loss: 5.0544 - val_loss: 7.5639\n",
            "Epoch 190/200\n",
            "29/29 [==============================] - 8s 275ms/step - loss: 5.0533 - val_loss: 7.5519\n",
            "Epoch 191/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0531 - val_loss: 7.5579\n",
            "Epoch 192/200\n",
            "29/29 [==============================] - 8s 276ms/step - loss: 5.0549 - val_loss: 7.5428\n",
            "Epoch 193/200\n",
            "29/29 [==============================] - 8s 268ms/step - loss: 5.0534 - val_loss: 7.5447\n",
            "Epoch 194/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0534 - val_loss: 7.5356\n",
            "Epoch 195/200\n",
            "29/29 [==============================] - 8s 269ms/step - loss: 5.0530 - val_loss: 7.5202\n",
            "Epoch 196/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0526 - val_loss: 7.4932\n",
            "Epoch 197/200\n",
            "29/29 [==============================] - 8s 266ms/step - loss: 5.0531 - val_loss: 7.4859\n",
            "Epoch 198/200\n",
            "29/29 [==============================] - 8s 266ms/step - loss: 5.0546 - val_loss: 7.4537\n",
            "Epoch 199/200\n",
            "29/29 [==============================] - 8s 271ms/step - loss: 5.0531 - val_loss: 7.5207\n",
            "Epoch 200/200\n",
            "29/29 [==============================] - 8s 270ms/step - loss: 5.0523 - val_loss: 7.5050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7JQ5ftoWRhTk",
        "outputId": "10f4a535-a7a6-46cc-a743-1aac7df452e7"
      },
      "source": [
        "# Visualizing the loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gc1dn+8e+zTV0ustwxtqmmxRiZToDQTK8BEkhIIHFIJ29CAmkkbxr5pRMS2osTQogTQk8oMb2EahsDBgy2wWC5d8mqW87vjzOSVrJkZFu7K+3en+vSpdXs7M7ZkXTPmWfOzJhzDhERKRyhXDdARESyS8EvIlJgFPwiIgVGwS8iUmAU/CIiBUbBLyJSYBT8IlthZn82sx/3ct4lZnbsjr6PSKYp+EVECoyCX0SkwCj4ZcALSiyXm9mrZtZgZjeb2Qgze9DM6s3sETMbkjb/aWb2upltNLMnzGxS2nP7m9nc4HX/AIq7LOsUM5sXvPZZM9tvO9v8WTNbZGbrzew+MxsdTDcz+42ZrTazOjN7zcz2CZ47yczeCNq2zMy+sV0rTAqegl/yxdnAccDuwKnAg8C3gWr83/lXAMxsd2AmcFnw3APAv8wsZmYx4B7gVmAo8M/gfQleuz8wA/gcUAXcANxnZkXb0lAz+wjwM+BcYBTwHvD34OnjgQ8Hn2NQMM+64Lmbgc855yqAfYDHtmW5Im0U/JIvfu+cW+WcWwY8DbzgnHvZOdcM3A3sH8x3HnC/c+5h51wc+CVQAhwKHAxEgd865+LOuTuAl9KWMR24wTn3gnMu6Zy7BWgJXrctLgBmOOfmOudagCuBQ8xsPBAHKoA9AXPOvemcWxG8Lg7sZWaVzrkNzrm527hcEUDBL/ljVdrjpm5+Lg8ej8b3sAFwzqWApcCY4LllrvOVC99Le7wz8PWgzLPRzDYCOwWv2xZd27AZ36sf45x7DLgW+AOw2sxuNLPKYNazgZOA98zsSTM7ZBuXKwIo+KXwLMcHOOBr6vjwXgasAMYE09qMS3u8FPiJc25w2lepc27mDrahDF86WgbgnLvGOXcAsBe+5HN5MP0l59zpwHB8Ser2bVyuCKDgl8JzO3CymR1jZlHg6/hyzbPAc0AC+IqZRc3sLODAtNfeBFxqZgcFB2HLzOxkM6vYxjbMBD5tZpOD4wM/xZemlpjZ1OD9o0AD0AykgmMQF5jZoKBEVQekdmA9SAFT8EtBcc69BVwI/B5Yiz8QfKpzrtU51wqcBXwKWI8/HnBX2mtnA5/Fl2I2AIuCebe1DY8A3wPuxO9l7AKcHzxdid/AbMCXg9YBvwie+wSwxMzqgEvxxwpEtpnpRiwiIoVFPX4RkQKj4BcRKTAKfhGRAqPgFxEpMJFcN6A3hg0b5saPH5/rZoiIDChz5sxZ65yr7jp9QAT/+PHjmT17dq6bISIyoJjZe91NV6lHRKTAKPhFRAqMgl9EpMAMiBp/d+LxOLW1tTQ3N+e6KRlVXFzM2LFjiUajuW6KiOSJARv8tbW1VFRUMH78eDpfTDF/OOdYt24dtbW1TJgwIdfNEZE8MWBLPc3NzVRVVeVt6AOYGVVVVXm/VyMi2TVggx/I69BvUwifUUSya0AH/wepa4qzul69ZRGRdHkd/PUtCdbWt2TkvTdu3Mgf//jHbX7dSSedxMaNGzPQIhGR3snr4DcgU3cb6Cn4E4nEVl/3wAMPMHjw4Ay1SkTkgw3YUT29YUCm7jNzxRVXsHjxYiZPnkw0GqW4uJghQ4awYMEC3n77bc444wyWLl1Kc3MzX/3qV5k+fTrQcfmJzZs3c+KJJ3L44Yfz7LPPMmbMGO69915KSkoy02ARkUDGgt/MZgCnAKudc/ukTf8y8EUgCdzvnPvmji7rh/96nTeW120xvTWZIp5MURbb9o+51+hKrjp17x6fv/rqq5k/fz7z5s3jiSee4OSTT2b+/Pntwy5nzJjB0KFDaWpqYurUqZx99tlUVVV1eo+FCxcyc+ZMbrrpJs4991zuvPNOLrzwwm1uq4jItshkj//P+HuT/qVtgpkdDZwOfMg512JmwzO4fC9Ld5Y88MADO421v+aaa7j77rsBWLp0KQsXLtwi+CdMmMDkyZMBOOCAA1iyZEl2GisiBS1jwe+ce8rMxneZ/HngaudcSzDP6r5YVk8981V1zayqa2bfMYMyPiyyrKys/fETTzzBI488wnPPPUdpaSlHHXVUt2Pxi4qK2h+Hw2Gampoy2kYREcj+wd3dgSPM7AUze9LMpvY0o5lNN7PZZjZ7zZo1WWxi71RUVFBfX9/tc5s2bWLIkCGUlpayYMECnn/++Sy3TkSkZ9k+uBsBhgIHA1OB281sonNbHoJ1zt0I3AhQU1OzXQWbtk6+cx2P+0pVVRWHHXYY++yzDyUlJYwYMaL9uWnTpnH99dczadIk9thjDw4++OC+XbiIyA7IdvDXAncFQf+imaWAYUBGuvRtWZ+pMv/f/va3bqcXFRXx4IMPdvtcWx1/2LBhzJ8/v336N77xjT5vn4hId7Jd6rkHOBrAzHYHYsDaTC3MgujvZodCRKRgZXI450zgKGCYmdUCVwEzgBlmNh9oBS7qrszTd43I2DuLiAxYmRzV87EensraQPVMl3pERAai/L5kQ9rBXRER8fI6+NXnFxHZUl4Hv3r8IiJbyu/gD75nIve397LMAL/97W9pbGzs4xaJiPROYQR/BpJfwS8iA1V+X5bZMtfnT78s83HHHcfw4cO5/fbbaWlp4cwzz+SHP/whDQ0NnHvuudTW1pJMJvne977HqlWrWL58OUcffTTDhg3j8ccf7/O2iYhsTX4E/4NXwMrXtphcmkoxMZ4iFgtv+zUbRu4LJ17d49Ppl2WeNWsWd9xxBy+++CLOOU477TSeeuop1qxZw+jRo7n//vsBfw2fQYMG8etf/5rHH3+cYcOGbVubRET6QF6XerJl1qxZzJo1i/33358pU6awYMECFi5cyL777svDDz/Mt771LZ5++mkGDRqU66aKiORJj7+Hnnlzc5x31jYwsbqc8qLMfVTnHFdeeSWf+9zntnhu7ty5PPDAA3z3u9/lmGOO4fvf/37G2iEi0hv53ePP4HjO9Msyn3DCCcyYMYPNmzcDsGzZMlavXs3y5cspLS3lwgsv5PLLL2fu3LlbvFZEJNvyo8ffg0wO50y/LPOJJ57Ixz/+cQ455BAAysvL+etf/8qiRYu4/PLLCYVCRKNRrrvuOgCmT5/OtGnTGD16tA7uikjW2UC4cmVNTY2bPXt2p2lvvvkmkyZN2urrGlsTLFq9mfFVZVSWRDPZxIzqzWcVEenKzOY452q6Ts/rUo8u2CAisqW8Dv726B8AezUiItkyoIP/g8pU7cd2s9CWTBkIpTgRGVgGbPAXFxezbt26rQbjQC/1OOdYt24dxcXFuW6KiOSRATuqZ+zYsdTW1rJmTc+3602kUqza1EJ8XZTS2MD8qMXFxYwdOzbXzRCRPDIw0xCIRqNMmDBhq/Ms39jEqbc+xs/P3pfzPjQuSy0TEenfBmyppzciIV/sSaQGarFHRKTv5XXwh4PgTyr4RUTa5XXwR0L+4yWSCn4RkTZ5HfzhcFupJ5XjloiI9B95Hfyq8YuIbKkggj+pUo+ISLu8Dv6wevwiIlvI6+A3M8Ih06geEZE0GQt+M5thZqvNbH43z33dzJyZZfyms+GQqccvIpImkz3+PwPTuk40s52A44H3M7jsdpGQkdSoHhGRdhkLfufcU8D6bp76DfBNsnTtNPX4RUQ6y2qN38xOB5Y5517pxbzTzWy2mc3e2oXYPkgkZDqBS0QkTdaC38xKgW8D3+/N/M65G51zNc65murq6u1ebjgUUo9fRCRNNnv8uwATgFfMbAkwFphrZiMzudBoWDV+EZF0Wbsss3PuNWB4289B+Nc459Zmcrmq8YuIdJbJ4ZwzgeeAPcys1swuydSytiaicfwiIp1krMfvnPvYBzw/PlPLTqcev4hIZ3l95i74SzPrWj0iIh3yPvjV4xcR6Szvgz+iUT0iIp3kffCrxy8i0lneB7/O3BUR6awAgj+k4ZwiImnyP/jDpnvuioikyfvg141YREQ6y+/gf+4PXLbm+zq4KyKSJr+Df8MSdmuerx6/iEia/A7+cIyIi6vHLyKSJs+DP0qYJImkDu6KiLTJ8+APevwKfhGRdvkd/KGo/55K5LYdIiL9SH4Hf9gHv6XiOW6IiEj/kefBHwPA1OMXEWmX58Hve/yhVGuOGyIi0n8USPCr1CMi0ibPg7+t1KPgFxFpUxDBH1KNX0SkXZ4Hf1DqcXGc09m7IiKQ78EfjOOPktD1ekREAvkd/EGpJ0ZC1+sREQnkefD7Hn+EpHr8IiKBPA9+3+OPmnr8IiJt8jz4VeMXEekqY8FvZjPMbLWZzU+b9gszW2Bmr5rZ3WY2OFPLB9qD39f4dYVOERHIbI//z8C0LtMeBvZxzu0HvA1cmcHlt5d6VOMXEemQseB3zj0FrO8ybZZzru1squeBsZlaPtBR4ydBIqngFxGB3Nb4LwYe7OlJM5tuZrPNbPaaNWu2bwmhCAAxHdwVEWmXk+A3s+8ACeC2nuZxzt3onKtxztVUV1dv34Lae/xJkqrxi4gAEMn2As3sU8ApwDEu09dRaK/xq8cvItImq8FvZtOAbwJHOucaM77AtOGcqvGLiHiZHM45E3gO2MPMas3sEuBaoAJ42Mzmmdn1mVo+kDacU6N6RETaZKzH75z7WDeTb87U8rqVPqpHwS8iAuT7mbuhMM5CRExn7oqItMnv4AdcKEqMpM7cFREJFETw6+CuiEiHggl+lXpERLz8D/5wVOP4RUTS5H/wh2LETGfuioi0yfvgJxzRcE4RkTT5H/yhmGr8IiJp8j/4wzGiJDWqR0QkUADBr1E9IiLpCib44zq4KyIC9DL4zeyrZlZp3s1mNtfMjs904/pEOEZUl2wQEWnX2x7/xc65OuB4YAjwCeDqjLWqD1lENX4RkXS9DX4Lvp8E3Oqcez1tWr9m4VgwnFOlHhER6H3wzzGzWfjg/4+ZVQADIkkt4sfxx9XjFxEBen89/kuAycA7zrlGMxsKfDpzzeo7oUgRURK0JAbEdkpEJON62+M/BHjLObfRzC4Evgtsylyz+o6FY0QtSauCX0QE6H3wXwc0mtmHgK8Di4G/ZKxVfSkcJUZCwS8iEuht8Ceccw44HbjWOfcH/L1z+79QlJglaU0mc90SEZF+obc1/nozuxI/jPMIMwsB0cw1qw8Fo3rU4xcR8Xrb4z8PaMGP518JjAV+kbFW9aXgevwKfhERr1fBH4T9bcAgMzsFaHbODZAaf9DjTyr4RUSg95dsOBd4EfgocC7wgpmdk8mG9ZlwlDAp4vF4rlsiItIv9LbG/x1gqnNuNYCZVQOPAHdkqmF9JuwPRSQTCn4REeh9jT/UFvqBddvw2twKxwBIxltz3BARkf6htz3+h8zsP8DM4OfzgAcy06Q+FgS/Syr4RUSg9wd3LwduBPYLvm50zn1ra68xsxlmttrM5qdNG2pmD5vZwuD7kB1pfK+E/LZNwS8i4vW6XOOcu9M59z/B1929eMmfgWldpl0BPOqc2w14NPg5s9p6/AkFv4gIfECpx8zqge4ua2mAc85V9vRa59xTZja+y+TTgaOCx7cATwBb3XPYYe3Br4O7IiLwAcHvnOvryzKMcM6tCB6vBEb0NKOZTQemA4wbN277lxiM6lGPX0TEy9nInODaPz1eJN85d6NzrsY5V1NdXb39CwqCn5SCX0QEsh/8q8xsFEDwffUHzL/j2kf1qNQjIgLZD/77gIuCxxcB92Z8iW09ftX4RUSADAa/mc0EngP2MLNaM7sEf4P248xsIXAs2bhhe9DjJ9WKry6JiBS23p7Atc2ccx/r4aljMrXMboV8j9/fcN0RDQ+Ie8SLiGTMwLjswo4IdwS/Ls0sIlIQwe9LPRF0310RESig4I/pmvwiIkBBBL8/jKFSj4iIVwDB73v8UUuqxy8iQiEFv3r8IiJAIQR/SKUeEZF0+R/8kWIAioir1CMiQgEFf4m1qscvIkIhBH8oRCpcTDEtCn4REQoh+IFUtJQSWmlR8IuIFEbwu0gJpdaiGr+ICAUS/ERLVOoREQkUSPCXUYIO7oqIQKEEf6yEUlpoTSRz3RIRkZwriOC3aCklqvGLiACFEvyxUopV6hERAQok+ENFpZTSTGtSt14UESmI4PelHvX4RUSgQIKf4AQuBb+ISAEFf7G10JpI5LolIiI5VyDBX0KEFMl4S65bIiKScwUS/KX+e7wpt+0QEekHCiP4Y0HwJxpz2w4RkX6gMII/6PGH1OMXESmU4C/x3xX8IiK5CX4z+5qZvW5m881sppkVZ3SBbT3+hIJfRCTrwW9mY4CvADXOuX2AMHB+RhcaBH84qRq/iEiuSj0RoMTMIkApsDyjSwtKPeFEc0YXIyIyEGQ9+J1zy4BfAu8DK4BNzrlZXeczs+lmNtvMZq9Zs2bHFhorAyCcVKlHRCQXpZ4hwOnABGA0UGZmF3adzzl3o3OuxjlXU11dvWMLDXr8kaR6/CIiuSj1HAu865xb45yLA3cBh2Z0iUGNP6rgFxHJSfC/DxxsZqVmZsAxwJsZXWJbj98p+EVEItleoHPuBTO7A5gLJICXgRszutBIMQ4jmlLwixSU956Fzath0qkQCnd+LpWEtQthzQKoX+nP8B88DqonQWkVhLMej1mTk0/mnLsKuCprCzQjHiomplE9IpnjHLQ2QFH5tr2utRHWvg2pBNQtg9ULYMU8KKuG6j2hpQ6q94Bdj/MnYZYMhnDUv7ZhHax9C8qGw6Cx/j3m/BlWvgqbV8E7T/j5qnbzr2ndDCP3g4Y1sPI1iG9liHesAoZOgPNuhSHjO6anklC33C/PbNs+az+Rv5u0LhLhYmKtzSRTjnBoYP6yRPqNZAL+cyVsqvXhvNtx8MIN8MY9sPNhcOBnYc9TOgK6aYMP9NoXYemLflpRBbgUvPWgD/d2BlW7wpL/QsumLZddPAjGHQJr3oIN73Z+LlwEyRYYtBOEIvCR78HQifD8dX6DES31G4XyETDlIhg9GYbvBZVjIN4A6xb7vYCmDf5r3m1w52fh0w9Cohn+822Yfxe01sOQCbDLR6B8OOx1Ogze2X++EftCWVVGVntfMef6/+0Ia2pq3OzZs3foPeqvnsSshl049tt3M6gk2kctE8kDTRshGYfyLqPnFj3qQ/qAi+CZ38BbD8HIfeFD50PtS/DsNb4nvWEJpOJgYdj/AnjnSdj4HlSMhppP+1783L/4kAcf6uGY733Hm2HiUTDpFIgUQ8VIH9RFFZBKQdN6//j952HpC1BUCctf9ssfPgnGToURe0PjOtj4vv++z9mw04F9s25euwPuvATG1Pj33rDEf8bhe8HbD8GKV6F5E+AgUgKJJv/ZxtT4vYmyYVA6DFrq/QZiyM5+PZQO83sTQyf6jU7XPQfn/Hxdy1PbyMzmOOdquk4vmB5/KlJCCS1sbkko+KWwpVLw3LW+LLPbcXD7J30wTfuZD/tNtT5QX7jOh8+TV/vX7XqcL4+8db//ueYSOOXXvme88GFfjhn1IV8KWfgwvHgjPP4T3/Oe+hn/+lH7+XDvjVDIByfAxCP9V7bte47fC3j7Id+WU3/X0Y5Dvui/N66H2TP8cYKJR/oN38rX/PybV8Oat/3G671ngo1EFxWjYNdj/V7K6Mn++13Tfflr+J4w7eew8yF9+rEKpse/8XeH8fLaMKO++G/2HFnZRy2TvPf+C77HN+1n/gAh+N7vhvfg+B/5f2iAVW/43lxwsmCfW/+uryuPP+yD53UOXr8bFj0CB3zK935bG+HR//W979bNsPixjvnLR/ivla/6Usjgcf6A567HwtHfhjm3+LLN7sf7UH/l77DiFf/5I0Vbb8uGJRCKwqAxO/Lp80PbMZBQ2B9jWLcY1i2Cd5+EJc/4DWib0irY91x//OL4H/u9mu3QU4+/cIL/j8exYGU9kYsfoGb80D5qmfSJ5jofoNt7oMw538MasY/vJXYnlYSnfuHrsjsdCMvmwIQP+93vrtYt9j24I74Ot30Uls32ZYxjf+APXP77a36+ql3hon/5UL22xvdkj/+x7yXWLff14YZ1MOWTvizx39/6+u9ux27757vhCFg5H866CTYugeXzYNzBPsDXLoJ9zoLDL4OSIXD3pfDKTN/TTiX8wcx4o/9cg3fyvdBjfwA7HeTnO/gLvu2v/N3vAVSO8fXzql3zemRLvxNv9r/PZXOg5uI+2VgWfKnHYqUUs5YNLbrvbr9SvxJ+fwAccxUcNL1jeirpR3ZU7eoP5qVbu9D/g2yq9aWFV/8BC2fB7tPgzBv8QbyuXrgBnvhZ52mTTvMjNtJtqoVbToO6Wn8Qr345TLval0Ae/p6fZ/wRfqPw17Ng9p+C9jm/a3/nJb5H98TVvtccisCC++HQL8EjP/AbkNOv9cvubvTL8pfh2Wt9TfvAz8ChX/E9wpWv+VEud33Gz1c+Ehb8238fuY+vt29e7csPr8yEgz4PR1/p27f4Mb8+L7zD9+JTqY4N5JgpHcuu+XTH4+F79vQbk0yJFsOeJ/mvDCuY4A/FSimlhaXNCv4dtm6xD6HiPiiZvfR/vvQw768dwf/OE34kRcNq2OccOOfmjvlfuwPu+YIfuWFhcEk/kmPKJ2He3+Avp8NnHvU9Ved86DWshcd+BLud4INxzVuwar4/4Lh2IQzbzb93wzq49Uxfhz3+J/DoD/3By6mfhYMuhdrZfgNz0KV+1MbOh/lRLBUj/djvSx6G/zsO/vkpsBB88l5fOplxAjz4TT8SBYN7Pu+/jv4OHPlNH/LOQelQ+HMw3rx6T7+heONeX2cvHwmXPu0/x+4nwh4nwqalfnokBvd/Hebe6nv44Zh/3+JBfi/g8Ms6r/Oe9oqkYBRM8EeKyimhlXoFf4eNSyHRAsN27f1rVi+AG4/0IyfO+OO2L3NTLdStgJ2m+jHZs2f4cFzxii9ZDB4H/7rM94bHTPHBV/9Tv6F58mp48ucw7lA483pfkljxiu9pD9nZjw6542J/UPGQL/ge/pM/98stHgSn/MbvPk88Ejav8XsKj/0I9jrDj2p58QZfu//EXTD+cNj1GIiVd5Q7dprqv9rsfYYP3LUL4bCv+DPEz/sL3HI6HPQ53x7wAf/C9XDGdX4j8daDMOdP8N/fweQLfDmppc63MVoM05/wn+21O/zGZ9NSOPaHvix12u87lj94XMfjA6f7jehrt/vPU6pypvSscIK/uJRia2FzSzzXTcmtZNwH7vrF8JczfAngs4/6cGne5AO9O5tqfQ3y7ul+PPPrd8OJP/eljHDM91If/xlUjvbD/8Af8Lzj076M07b7eu8XfY/+iG/4oXqN63zd+q7pMP9Ovxex4V244E4/3O3tKb6MsfF9ePM+mHyhH0nSdlBx7AEdbdz7LJg3048kWfQILH7Uz3/w5/3IifSx1eXVfi/hxRv9xgX8Zzn3Vh/64OvyWzPpNHjgct8j3+0EP23oRLjs1c7HKz78DTj8ax1D8/Y5y5/8c/NxcNs5PvSnXOTbfPb/+ecA9vuoHx/+3jMw/sNbb0v1HjDhSF8W2n+Lax6KdFI4wV9URiktbC7kHn/tHPjnRT7kQxE/zjrR7AOoeRNgfnzxxCN9HfjpX/qRHm2ntbc57Ku+t/riTb6XudOBcNSVvkdeOdYHKvgTfNYs8Ms8f6YfyvfuU36ep3/p59n7LNj3o37kyDO/gWQrTDza97bN/Akyz13ryzrH/xgO+VLPB4HN4KRf+DCtXwkHf9GPPOlpLPQxV8Fux/uNVaTYjxHvOpZ9a8qH+43Eilc6jxvvrn1d2zB2qj/Qu+o1P8zxtGu6X0Yk5tdBbxxzFcy9pffzS8EqmOC3kiGUWQuNTQ25bkrvNKyFJU/7A5Zt9wx+9yk/7OvIK/zp6Ctf9cHVXdA452vZ6xbCHifD+8/6Hn7FKF96aNrgR3PUr4C/XwCHftmfoHPP5+Hz//W98sd/4g+uDh7nw7y0ypcqJhwJb/7LlyHA9/7XLvKP62p9EG5e7d/jqCvhzX/7g54Hf973ji/4p+/lDhrb0bs94mvw3B9g1GRfQ2/7TEdd6fdSjrmqc5mlJ0MnwJfn9G4dF5X7USw74tTf+WMD4W08N8TMH9O478t+j6AvjD2g8x6QSA8KZjgn82bCPZdy9a63ccWFp/RNwzLl8Z/C07/yB+omX+Br6amUHzK4frEP4cWP+577+CN8L7etLNG43g9bfOsBP4Ya4MDPwdsP+l7+Zx7dsv7rnA+iZXPh5uP9WPH6lX76F57rvsf81C99ffy03/ue+vp3/IHY1+/yI1EW/NuH/Bde8KWb6w7zZ3dW7wlffCGjq2/AcM6vt6pdct0SyVMFP5yzrWdZ1LAis8tJJeHt//ge8ZRP9u6Mu9YGX2opGeJPRX/y5762WzYcXrrJ97CLK33oj9zPj0YpHux77s9eC388xG8gTv0tPHSFr5Xv8hEfwCtf9QctMbj4oe4P+rX1rsdM8T3Ye7/gfz7nTz2XSQ7/mt/bGLWfP9Pw/q/7seF1y3xN3qXgE3f7UkX1Hv7g59O/gr3P3I6VmqfMFPqSE4UT/IN3AqCsOcPB//D3fU0afH37s4/1XJNOJf1476d/5Yclxir8QcshE+CM6/1B01Xz4Z5L/ZmVlWPhklnw/B99CWjE3v60+Wd+7ZfZvNH3tA+7DI77YccywjF/dcFxB39w+/e/ABrX+qGLe53R83yhsA99gF2Ohq/M9Y/3OBHefw72O79zrfnDl3ecui8iOVU4wV8xmhRGRUsGg3/jUj9KZL/zYPQUeOhb/sJSXQO3fpUP6oWz/MZh34/6Md61L/m6+Bl/9NcGB/j47fDQlX6c+wk/9fX+I77e8V5lVXDCT/ywzJdu8nsNh3+t4/lQ2JeCtsVhX92ujw/4PY8N78FHvtt5erTEn/4vIjlXOMEfiVEXrmJw66q+e0/n4L3/+gOgZcM7zgw95vs+gJ+82l8fZedDfc08lfTD/V68wV8RcaeD/FDG/c71r5t6yZbLKK6EM/7gD4wtvCkAABCGSURBVACmXxO8qxN+4k+E2v2E7s9czZayYX64pYj0W4UT/MCmopFUNa3e9hc2b/KjacYf4WvYj/4Ipl7sR83859uA+ZNvmjf6kTJtI1Wmfhae+n/+zMyyYJjgK3/zBzgv+tcHjxNPN3TC1p+PFPmTmkREPkBBBf/mopGMaJi/9ZmS8Y6heYlWX5L57299+JeP8Bfkaq33o2bAD5UcuY/fIIw/ovMJUEd+01+wa8j4jhOOGtb5jYQufiUiOVJQ6dNUOopd1z9JMpkkHO5mtMqcW/zolEO/5K/R8uzvYc2b/tooHzrPX+grlfSn3j97jb+06jk3d4yz7yoc9SNa0vXzO/OISP4rqOBvLR9DkcWp27CSymHBJU9TSVj9hh8z/+C3/NmYz/zGP1e1K3zsH7DHNP9z+lDEns60FBHp5woq+JMVvvbetGaJD/5Uyl/U6417/AwlQ/0JTvUrfMlnpwMH7M2URUR6UlDB33bQNblmIRQHdxJ64x5/uYJYuT9RqnKU/xIRyVMFFfyhIf4ytqMfSxunfsiX4LgfqWcvIgWjoIK/tGIItyaO5ehJIxg79XR/hURdt1xECkxBBX9FSZTvJS7mmn32Z+zuo3PdHBGRnCioe7CVF/nx+QV9TX4RKXg5CX4zG2xmd5jZAjN708x6cQnLHVde7HdwCv4uXCJS0HJV6vkd8JBz7hwziwGl2VhoWSxMLBJi+cbmbCxORKRfynrwm9kg4MPApwCcc61Aa5aWzUEThvL0wjXZWJyISL+Ui1LPBGAN8Ccze9nM/s/MyrrOZGbTzWy2mc1es6bvgvrI3atZvKaBpesb++w9RUQGklwEfwSYAlznnNsfaACu6DqTc+5G51yNc66munobboD9AY7aYzgAT76tXr+IFKZcBH8tUOuca7vx6h34DUFW7FJdxtghJTzxloJfRApT1oPfObcSWGpmbZetPAZ4I1vLNzOO2qOa/y5ayytLN2ZrsSIi/UauxvF/GbjNzF4FJgM/zebCP3P4RIaWxfjoDc/x43+/wdz3N+Ccy2YTRERyxgZC4NXU1LjZs2f36Xuub2jlu/e8xsNvrCKedNTsPIRT9hvFTkNLGTuklLFDSigrKqgTm0Ukz5jZHOdczRbTCzX429Q3x7n75WVc/8Rilm/qPL6/uqKIo3av5qCJVYwZXMLowcUMLYtRHA0TDRfUSc8iMgAp+D+Ac461m1tZuqGR2g1NLF3fyFsr63n8rdXUd7nEQzhknDNlLP9z/O6MqCzOaLtERLZXT8GvWkbAzKiuKKK6oogp44a0T48nUyzb0MTyjU3UbmxiU2OcJesa+MdLS7l9zlL2Hl3J4btWc+yk4Ryw8xBMl3cWkX5OPf7ttGRtA/e9spxnFq1l7nsbSKQc+48bzMn7jqJm/FD2GlVJLKJykIjkjko9GbS5JcHdLy/jT8+8yztrGwAojoYYX1VGaSzM0g1NDCsv4kNjBzGsvIghZTFGVhZz9J7VlMa00yUimaHgz5JVdc3MXrKBue9v4L11jTS0JBg7pISVdc28uaKODY1xkim/ziuKIhw0cSjDK4sZUVHMHiMrOGK3YRRHwzTHkySSjsqSiMpHIrJdVOPPkhGVxZy83yhO3q/7+/amUo765gQLVtbxj9lLeWN5HXPf38j6hu6vUzeoJMruI8oZN7SMnatKSTnHa7Wb2Ht0JadNHsPEYWWEQoZzThsIEekV9fj7iZZEkrnvbeSFd9dhGCWxECEzFq9pYPHqzby/vpGVdc2YwYSqMt5d14Bz/lLTAEnn2H+nIYRDxqq6ZpoTSarLi9hteAXxZIpBpVF2HV5OeVGElkSKxpYE46pKGVFZjHNQ1xzHMMZVlTKysphwSBsRkYFOPf5+rigS5pBdqjhkl6oe52mOJ4knU1QUR1m+sYmn3l7DgpX1hENGMuWY+/4GQmbsOryc4miYZRuaeHTBaooiIdY3tNIUT/aqLbFwiKFlMVLOkXKQcg7nHCMHlTBmcAktiSRNrUlakylCZgwujTK0LEZLIsWwshi7jahgfUMriWSKoWUxhgTPLV3fSGsiRWVJlEmjKmiJp6hvThCLhBheUUR5cYTlG5tpbE0QDYeYNKqSokiIlkSKEZVFbGyMs2xjEzsHGyft4YhsHwX/AFIcDVMc9T380YNLOP/Acb1+bSrlWFnXTFM8STQUoiQWZsm6BtZt9iWmQSVRkinH++sbeX99I+sbWgiZEQoZbZ3/2g1N1G5opCgapjQaprw4QjLlWLu5hbdX1lMcDbOyrpnGVr+BMYP0HcqQQSwSojme2uF1ETKoKI5SURwhFgnhgg1UU2uSuuY4ITOi4RCxSIhYOEQ0bMQiIaJh/xULh6gsiVAai9AcbBCjwXyRcKj9cTQcIhI2QmYY+O8GpbEIw8pjxJOOhpYEja1JiqMhKoqjlBdHcM4RTzriyRTRcAjnHMs2NuEclBdFKCuKUF4coTT4fTogmUqxsTGOGQwqiREJGaEQGEbKOUIhoyJ4bXE0jHMOhz8HJZmCZMr5r2724i34fEVR/9nDIf+ZQuaHMoeM4GfDQv7x1ljw+/WP/YOuLzHb8jlrf87a36OnDXgq5UikXNABcTgXtDFoXzj4+8y0eDJFMuWIhUNZWV42KPgLRChkjB5c0mladUVRny8nGWxgqspiRMMhNjXFWd/QQiQUYuyQEiLhEHXNcd5aWU9ZLEJlSYTWRIqVdc3UNycYPaiEypIIDS1J3lxRR9L5f7iVdc1UFkcZM6SE99c1sLq+hbqmOHXNCeLBnkfI/J7ToNIozjlaEylag/BtTaTav7cG35dtbKapNdG+MY0nUyRSjngiRTzlX5dIOlqTKWjb84H2EJK+Ye0bnY6NQCKZItXLdRwyf1KlBRuDnpbRdaPjf+gyPW0DZWYkkinq0k7gDIeMWNChCIc6bzjDZp3eo225/v2sm2np81nnadbRhp+euS8HThjau5XRSwp+6VPhkDEmbQMztCzG0LJYp3kqi6NMHd/5D3lidfkW77XX6MoeltJ392fYXg0tCdZtbiUaMcqKfM+9OZGivjnO5uYEZj4gohEjnvA91lGDi4mGQjS0JmhoSbK5Jd6xd4TvyQ4qiQKwsTHevoFJOUc4ZO17Fw0tCZoTSQxr7zGHzQiH2r46Bw3494gnU7Qk/JdLK+OlnN9rSKXSp7kt3qONo2PD15bNHT+7Tj/7x67LPB1tcm3LTm8LjmjI72n5vZ6OPa72+YI9m5Qj7bGf3nUPwrnO7U1va7ftTJsnZDCkLEYsEiKecLQmk77zkEh1anMq2DPptLfV5fN2XUbHtK7rMliLwYSyojB9TcEvsh3KgpJLuvJwiPKiCAza+mt9iSoK9Hy5j7FDenxKZIfp1FIRkQKj4BcRKTAKfhGRAqPgFxEpMAp+EZECo+AXESkwCn4RkQKj4BcRKTAD4uqcZrYGeG87Xz4MWNuHzekr/bVd0H/bpnZtm/7aLui/bcu3du3snNviVPcBEfw7wsxmd3dZ0lzrr+2C/ts2tWvb9Nd2Qf9tW6G0S6UeEZECo+AXESkwhRD8N+a6AT3or+2C/ts2tWvb9Nd2Qf9tW0G0K+9r/CIi0lkh9PhFRCSNgl9EpMDkdfCb2TQze8vMFpnZFTlsx05m9riZvWFmr5vZV4PpPzCzZWY2L/g6KQdtW2JmrwXLnx1MG2pmD5vZwuB7Vm8LYmZ7pK2TeWZWZ2aX5Wp9mdkMM1ttZvPTpnW7jsy7Jvibe9XMpmS5Xb8wswXBsu82s8HB9PFm1pS27q7Pcrt6/N2Z2ZXB+nrLzE7Icrv+kdamJWY2L5iezfXVUz5k7m/M35Ys/76AMLAYmAjEgFeAvXLUllHAlOBxBfA2sBfwA+AbOV5PS4BhXab9P+CK4PEVwM9z/HtcCeycq/UFfBiYAsz/oHUEnAQ8iL9l6sHAC1lu1/FAJHj887R2jU+fLwfrq9vfXfB/8ApQBEwI/mfD2WpXl+d/BXw/B+urp3zI2N9YPvf4DwQWOefecc61An8HTs9FQ5xzK5xzc4PH9cCbwJhctKWXTgduCR7fApyRw7YcAyx2zm3vmds7zDn3FLC+y+Se1tHpwF+c9zww2MxGZatdzrlZzrm2u4M/D4zNxLK3tV1bcTrwd+dci3PuXWAR/n83q+0yf6Pec4GZmVj21mwlHzL2N5bPwT8GWJr2cy39IGzNbDywP/BCMOlLwe7ajGyXVAIOmGVmc8xsejBthHNuRfB4JTAiB+1qcz6d/xlzvb7a9LSO+tPf3cX4nmGbCWb2spk9aWZH5KA93f3u+sv6OgJY5ZxbmDYt6+urSz5k7G8sn4O/3zGzcuBO4DLnXB1wHbALMBlYgd/VzLbDnXNTgBOBL5rZh9OfdH7fMidjfs0sBpwG/DOY1B/W1xZyuY56YmbfARLAbcGkFcA459z+wP8AfzOzyiw2qV/+7tJ8jM4djKyvr27yoV1f/43lc/AvA3ZK+3lsMC0nzCyK/6Xe5py7C8A5t8o5l3TOpYCbyNAu7tY455YF31cDdwdtWNW26xh8X53tdgVOBOY651YFbcz5+krT0zrK+d+dmX0KOAW4IAgMglLKuuDxHHwtffdstWkrv7v+sL4iwFnAP9qmZXt9dZcPZPBvLJ+D/yVgNzObEPQczwfuy0VDgvrhzcCbzrlfp01Pr8udCczv+toMt6vMzCraHuMPDM7Hr6eLgtkuAu7NZrvSdOqF5Xp9ddHTOroP+GQw8uJgYFPa7nrGmdk04JvAac65xrTp1WYWDh5PBHYD3sliu3r63d0HnG9mRWY2IWjXi9lqV+BYYIFzrrZtQjbXV0/5QCb/xrJx1DpXX/ij32/jt9bfyWE7Dsfvpr0KzAu+TgJuBV4Lpt8HjMpyuybiR1S8Arzeto6AKuBRYCHwCDA0B+usDFgHDEqblpP1hd/4rADi+HrqJT2tI/xIiz8Ef3OvATVZbtcifP237e/s+mDes4Pf8TxgLnBqltvV4+8O+E6wvt4CTsxmu4LpfwYu7TJvNtdXT/mQsb8xXbJBRKTA5HOpR0REuqHgFxEpMAp+EZECo+AXESkwCn4RkQKj4BfJMDM7ysz+net2iLRR8IuIFBgFv0jAzC40sxeD66/fYGZhM9tsZr8JrpP+qJlVB/NONrPnreO6923XSt/VzB4xs1fMbK6Z7RK8fbmZ3WH+Wvm3BWdriuSEgl8EMLNJwHnAYc65yUASuAB/BvFs59zewJPAVcFL/gJ8yzm3H/7sybbptwF/cM59CDgUf6Yo+CsuXoa/zvpE4LCMfyiRHkRy3QCRfuIY4ADgpaAzXoK/KFaKjot3/RW4y8wGAYOdc08G028B/hlc92iMc+5uAOdcM0Dwfi+64Fow5u/yNB54JvMfS2RLCn4Rz4BbnHNXdppo9r0u823vNU5a0h4n0f+e5JBKPSLeo8A5ZjYc2u93ujP+f+ScYJ6PA8845zYBG9JuzvEJ4Enn755Ua2ZnBO9RZGalWf0UIr2gXocI4Jx7w8y+i78bWQh/BccvAg3AgcFzq/HHAcBfJvf6INjfAT4dTP8EcIOZ/W/wHh/N4scQ6RVdnVNkK8xss3OuPNftEOlLKvWIiBQY9fhFRAqMevwiIgVGwS8iUmAU/CIiBUbBLyJSYBT8IiIF5v8DAyFy6E57n5IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44wv3zsUymd0"
      },
      "source": [
        "# This piece of code is referred from \"https://www.programmersought.com/article/3918650197/\"\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22XIMTJgymgQ"
      },
      "source": [
        "# This piece of code is referred from the website \"https://www.programmersought.com/article/3918650197/\"\n",
        "\n",
        "# generate a description for an image\n",
        "def description(model, tokenizer, photo, maximum_length):\n",
        "    in_text = 'START'\n",
        "    for i in range(900):\n",
        "        seq = tokenizer.texts_to_sequences([in_text])[0][-100:]\n",
        "        seq = pad_sequences([seq], maxlen=maximum_length)\n",
        "        yhat = model.predict([photo,seq], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        \n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        print(' ' + word, end='')\n",
        "        if word == 'END':\n",
        "            break\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrkwFlfhymi3",
        "outputId": "8ca691c3-4e11-4af2-f78e-e564ba46910c"
      },
      "source": [
        "# Testing \n",
        "test = img_to_array(load_img('/content/drive/MyDrive/HTML/images/87.jpg', target_size=(299, 299)))\n",
        "test = np.array(test, dtype=float)\n",
        "test = preprocess_input(test)\n",
        "test_features = densenet.predict(np.array([test]))\n",
        "description(model, token, np.array(test_features), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a <li><a"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_-hhfN8ymln"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m5g_aR_ymoo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iME--MJ_ymqz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}